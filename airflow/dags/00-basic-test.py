# Airflow Modules
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.utils.dates import days_ago

# External modules
from modules import noaa_isd
import logging
from datetime import timedelta, datetime
import glob
import os
import shutil

# Global variables
YEAR = datetime.now().strftime("%Y")
TODAY = datetime.now().strftime("%Y-%m-%d")
RAW_FILES_DIRECTORY = f"/mounts/shared-volume/shared/retail-data/raw-data"
DELTA_FILES_DIRECTORY = f"/mounts/shared-volume/shared/retail-data/delta-tables/"

def extract():
        # Folder to save the raw files. Create it if it does not exist    
    shutil.rmtree(f"{RAW_FILES_DIRECTORY}")    
    if not os.path.exists(f"{RAW_FILES_DIRECTORY}"):
        os.makedirs(f"{RAW_FILES_DIRECTORY}")    

    
    
    s3_hook = S3Hook(aws_conn_id='aws_sedev1_df')
    s3_hook.get_conn()
    files = s3_hook.list_keys('speed-download')
    string = f"{RAW_FILES_DIRECTORY}/"

    for file in files:
        s3_hook.download_file(
            key=file,
            bucket_name=('speed-download'),
            local_path=string,
            preserve_file_name=True,
            use_autogenerated_subdir=False
        )

def transform():
    s3_hook = S3Hook(aws_conn_id='aws_sedev1_df')
    s3_hook.get_conn()
    files = os.listdir(f"{RAW_FILES_DIRECTORY}")
    string = f"{RAW_FILES_DIRECTORY}/"
    new_files = [string + x for x in files]
    for file in new_files:
        s3_hook.load_file(
            filename=file,
            key='speed/'.join(file.split('/')[-2:]),
            bucket_name='speed-upload',
            replace=True
        )
        
def load():
    s3_hook = S3Hook(aws_conn_id='aws_sedev1_df')
    s3_hook.get_conn()
    files = os.listdir(f"{RAW_FILES_DIRECTORY}")
    string = f"{RAW_FILES_DIRECTORY}/"
    new_files = [string + x for x in files]
    for file in new_files:
        s3_hook.load_file(
            filename=file,
            key='speed/'.join(file.split('/')[-2:]),
            bucket_name='speed-upload',
            replace=True
        )

################# DAG #################

# DAG configuration
local_workflow = DAG(
    "00-basic-test",
    schedule_interval="1 0 * * *", # Run at 00:01 Everyday
    start_date = days_ago(1),    
    dagrun_timeout=timedelta(minutes=60),
    catchup=False
)

with local_workflow:  
    # Download the objects from the S3 Bucket
    task1 = BashOperator(
        task_id = "ExtractData",
        bash_command="ls -ltr /mnt/shared/retail-data/raw-data"
        #python_callable=extract
    )
    
    task2 = BashOperator(
        task_id = "TransformData",
        bash_command = 'ls -ltr /mnt/user'
        #python_callable=transform
    )

    task3 = BashOperator(
        task_id='LoadData',
        bash_command = 'ls -ltr /mnt/user/ezua-tutorials/images'
        #python_callable=load
    )
    
    task4 = BashOperator(
        task_id='UploadData',
        bash_command = 'pip list | grep pandas'
        #python_callable=load
    )


    task1 >> task2 >> task3 >> task4
